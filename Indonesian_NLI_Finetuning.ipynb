{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Indonesian NLI Models with Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fine-tunes two models (IndoRoBERTa and IndoBERT) on the Indonesian NLI (IndoNLI) dataset. It integrates MLFlow, TensorBoard, and DVC for comprehensive experiment tracking. It is designed to run on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio mlflow transformers datasets scikit-learn accelerate evaluate tensorboard dvc[gdrive] pyngrok Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from getpass import getpass\n",
    "from pyngrok import ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "Set `USE_SMALL_SUBSET` to `True` for a quick test run. Set `NUM_TRAIN_EPOCHS` to control the training duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True for a quick run with a small portion of the data\n",
    "USE_SMALL_SUBSET = True\n",
    "\n",
    "# Number of training epochs\n",
    "NUM_TRAIN_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading IndoNLI dataset...\")\n",
    "dataset = load_dataset(\"afaji/indonli\", trust_remote_code=True)\n",
    "\n",
    "if USE_SMALL_SUBSET:\n",
    "    print(\"Using a small subset of the data for a quick run.\")\n",
    "    dataset[\"train\"] = dataset[\"train\"].select(range(100))\n",
    "    dataset[\"validation\"] = dataset[\"validation\"].select(range(50))\n",
    "    dataset[\"test_lay\"] = dataset[\"test_lay\"].select(range(50))\n",
    "\n",
    "print(\"Dataset loaded.\")\n",
    "print(dataset)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    # --- Fine-tuning indoreoberta ---\n",
    "    print(\"\n--- Fine-tuning indoreoberta ---\")\n",
    "    model_name_roberta = \"flax-community/indonesian-roberta-base\"\n",
    "    tokenizer_roberta = AutoTokenizer.from_pretrained(model_name_roberta)\n",
    "    model_roberta = AutoModelForSequenceClassification.from_pretrained(model_name_roberta, num_labels=3)\n",
    "\n",
    "    tokenized_datasets_roberta = dataset.map(lambda x: preprocess_function(x, tokenizer_roberta), batched=True)\n",
    "\n",
    "    training_args_roberta = TrainingArguments(\n",
    "        output_dir=\"./results_indoreoberta\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=[\"mlflow\", \"tensorboard\"],\n",
    "    )\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer_roberta = Trainer(\n",
    "        model=model_roberta,\n",
    "        args=training_args_roberta,\n",
    "        train_dataset=tokenized_datasets_roberta[\"train\"],\n",
    "        eval_dataset=tokenized_datasets_roberta[\"validation\"],\n",
    "        tokenizer=tokenizer_roberta,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    mlflow.start_run(run_name=\"indoreoberta-finetune\")\n",
    "    trainer_roberta.train()\n",
    "    eval_results_roberta = trainer_roberta.evaluate()\n",
    "    mlflow.log_metrics(eval_results_roberta)\n",
    "    os.makedirs(\"metrics\", exist_ok=True)\n",
    "    with open(\"metrics/roberta_metrics.json\", \"w\") as f:\n",
    "        json.dump(eval_results_roberta, f, indent=4)\n",
    "    mlflow.end_run()\n",
    "\n",
    "    # --- Fine-tuning indoBERT ---\n",
    "    print(\"\n--- Fine-tuning indoBERT ---\")\n",
    "    model_name_bert = \"indobenchmark/indobert-base-p1\"\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(model_name_bert)\n",
    "    model_bert = AutoModelForSequenceClassification.from_pretrained(model_name_bert, num_labels=3)\n",
    "\n",
    "    tokenized_datasets_bert = dataset.map(lambda x: preprocess_function(x, tokenizer_bert), batched=True)\n",
    "\n",
    "    training_args_bert = TrainingArguments(\n",
    "        output_dir=\"./results_indobert\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=[\"mlflow\", \"tensorboard\"],\n",
    "    )\n",
    "\n",
    "    trainer_bert = Trainer(\n",
    "        model=model_bert,\n",
    "        args=training_args_bert,\n",
    "        train_dataset=tokenized_datasets_bert[\"train\"],\n",
    "        eval_dataset=tokenized_datasets_bert[\"validation\"],\n",
    "        tokenizer=tokenizer_bert,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    mlflow.start_run(run_name=\"indobert-finetune\")\n",
    "    trainer_bert.train()\n",
    "    eval_results_bert = trainer_bert.evaluate()\n",
    "    mlflow.log_metrics(eval_results_bert)\n",
    "    with open(\"metrics/bert_metrics.json\", \"w\") as f:\n",
    "        json.dump(eval_results_bert, f, indent=4)\n",
    "    mlflow.end_run()\n",
    "\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow UI\n",
    "We will use `ngrok` to create a public URL for the MLFlow UI running in the Colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate open tunnels if any\n",
    "ngrok.kill()\n",
    "\n",
    "# Get your ngrok authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "NGROK_AUTH_TOKEN = getpass('Enter your ngrok authtoken: ')\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Start MLFlow UI in the background\n",
    "get_ipython().system_raw('mlflow ui --port 5000 &')\n",
    "\n",
    "# Open a tunnel to the MLFlow UI\n",
    "public_url = ngrok.connect(5000)\n",
    "print(f\"MLFlow UI is running at: {public_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "Launch TensorBoard using the magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DVC\n",
    "Initialize DVC and track the generated metrics files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc init -f\n",
    "!dvc add metrics/roberta_metrics.json metrics/bert_metrics.json\n",
    "\n",
    "# To track with Google Drive, you would typically run:\n",
    "# !dvc remote add -d gdrive gdrive://<your_folder_id>\n",
    "# !dvc push\n",
    "\n",
    "# For now, let's just see the status\n",
    "!dvc status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
