{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Indonesian NLI Models on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fine-tunes two models (IndoRoBERTa and IndoBERT) on the Indonesian NLI (IndoNLI) dataset. It is adapted to run on Kaggle and integrates MLFlow, TensorBoard, and DVC for comprehensive experiment tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio mlflow transformers datasets scikit-learn accelerate evaluate tensorboard dvc[gdrive] Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "Set `USE_SMALL_SUBSET` to `True` for a quick test run. Set `NUM_TRAIN_EPOCHS` to control the training duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SMALL_SUBSET = True\n",
    "NUM_TRAIN_EPOCHS = 1"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Credentials with Kaggle Secrets\n",
    "Before running the next cell, you need to add your GitHub Personal Access Token as a secret to Kaggle:\n",
    "1. In the notebook editor, go to **Add-ons > Secrets**.\n",
    "2. Add a new secret with the label **`GIT_TOKEN`** and paste your token as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "GIT_TOKEN = user_secrets.get_secret(\"GIT_TOKEN\")\n",
    "os.environ['GIT_TOKEN'] = GIT_TOKEN\n",
    "\n",
    "# Configure Git\n",
    "os.system(f\"git config --global user.email 'your_email@example.com'\")\n",
    "os.system(f\"git config --global user.name 'fabhiansan'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://fabhiansan:{GIT_TOKEN}@github.com/fabhiansan/logging_experiment.git\n",
    "%cd logging_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading IndoNLI dataset...\")\n",
    "dataset = load_dataset(\"afaji/indonli\", trust_remote_code=True)\n",
    "\n",
    "if USE_SMALL_SUBSET:\n",
    "    print(\"Using a small subset of the data for a quick run.\")\n",
    "    dataset[\"train\"] = dataset[\"train\"].select(range(100))\n",
    "    dataset[\"validation\"] = dataset[\"validation\"].select(range(50))\n",
    "    dataset[\"test_lay\"] = dataset[\"test_lay\"].select(range(50))\n",
    "\n",
    "print(\"Dataset loaded.\")\n",
    "print(dataset)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same training function from the previous notebook\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow\n",
    "After the experiment, the `mlruns` directory is created. We will zip it for easy download. You can then unzip it on your local machine and run `mlflow ui` to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r /kaggle/working/mlruns.zip mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DVC and Git\n",
    "Track metrics with DVC and push all changes to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc init -f\n",
    "# Set the new GitHub repo as the DVC remote\n",
    "!dvc remote add --default origin https://github.com/fabhiansan/logging_experiment-storage\n",
    "\n",
    "# Configure DVC to use the Git token for authentication\n",
    "!dvc remote modify origin auth basic\n",
    "!dvc remote modify origin user fabhiansan\n",
    "!dvc remote modify origin password $GIT_TOKEN\n",
    "\n",
    "!dvc add metrics/roberta_metrics.json metrics/bert_metrics.json\n",
    "!dvc push\n",
    "\n",
    "!git add .\n",
    "!git commit -m \"Kaggle experiment run with metrics\"\n",
    "# We use the GIT_TOKEN environment variable for the push URL\n",
    "!git push https://fabhiansan:$GIT_TOKEN@github.com/fabhiansan/logging_experiment.git main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
